{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# HF_MODEL_NAME = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "HF_MODEL_NAME = \"prajjwal1/bert-tiny\"\n",
    "MODELS_FOLDER = './models'\n",
    "MODEL_SAVE_NAME = 'bert-tiny'\n",
    "MODEL_LOAD_NAME = None\n",
    "USE_WANDB = True\n",
    "\n",
    "RS = 42  # Random state\n",
    "TRAIN_FRACTION = 0.9\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1. Json to Dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                         id                     title  \\\n0  5733be284776f41900661182  University_of_Notre_Dame   \n1  5733be284776f4190066117f  University_of_Notre_Dame   \n2  5733be284776f41900661180  University_of_Notre_Dame   \n3  5733be284776f41900661181  University_of_Notre_Dame   \n4  5733be284776f4190066117e  University_of_Notre_Dame   \n\n                                             context  \\\n0  Architecturally, the school has a Catholic cha...   \n1  Architecturally, the school has a Catholic cha...   \n2  Architecturally, the school has a Catholic cha...   \n3  Architecturally, the school has a Catholic cha...   \n4  Architecturally, the school has a Catholic cha...   \n\n                                            question  \\\n0  To whom did the Virgin Mary allegedly appear i...   \n1  What is in front of the Notre Dame Main Building?   \n2  The Basilica of the Sacred heart at Notre Dame...   \n3                  What is the Grotto at Notre Dame?   \n4  What sits on top of the Main Building at Notre...   \n\n                               answer_text  answer_start  answer_end  \n0               Saint Bernadette Soubirous           515         541  \n1                a copper statue of Christ           188         213  \n2                        the Main Building           279         296  \n3  a Marian place of prayer and reflection           381         420  \n4       a golden statue of the Virgin Mary            92         126  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer_text</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5733be284776f41900661182</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>To whom did the Virgin Mary allegedly appear i...</td>\n      <td>Saint Bernadette Soubirous</td>\n      <td>515</td>\n      <td>541</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5733be284776f4190066117f</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What is in front of the Notre Dame Main Building?</td>\n      <td>a copper statue of Christ</td>\n      <td>188</td>\n      <td>213</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5733be284776f41900661180</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n      <td>the Main Building</td>\n      <td>279</td>\n      <td>296</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5733be284776f41900661181</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What is the Grotto at Notre Dame?</td>\n      <td>a Marian place of prayer and reflection</td>\n      <td>381</td>\n      <td>420</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5733be284776f4190066117e</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What sits on top of the Main Building at Notre...</td>\n      <td>a golden statue of the Virgin Mary</td>\n      <td>92</td>\n      <td>126</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the raw json\n",
    "url = 'training_set.json'\n",
    "with open(url, 'r') as file:\n",
    "    raw = json.load(file)['data']\n",
    "data = []\n",
    "for topic in raw:\n",
    "    for paragraph in topic['paragraphs']:\n",
    "        for question in paragraph['qas']:\n",
    "            assert len(question['answers']) == 1\n",
    "            answer = question['answers'][0]\n",
    "            data.append((\n",
    "                question['id'],\n",
    "                topic['title'],\n",
    "                paragraph['context'],\n",
    "                question['question'],\n",
    "                answer['text'],\n",
    "                answer['answer_start'],\n",
    "                answer['answer_start'] + len(answer['text']),\n",
    "            ))\n",
    "dataset = pd.DataFrame(data,\n",
    "                       columns=('id', 'title', 'context', 'question', 'answer_text', 'answer_start', 'answer_end'))\n",
    "dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87599 entries, 0 to 87598\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            87599 non-null  string\n",
      " 1   title         87599 non-null  string\n",
      " 2   context       87599 non-null  string\n",
      " 3   question      87599 non-null  string\n",
      " 4   answer_text   87599 non-null  string\n",
      " 5   answer_start  87599 non-null  Int64 \n",
      " 6   answer_end    87599 non-null  Int64 \n",
      "dtypes: Int64(2), string(5)\n",
      "memory usage: 4.8 MB\n"
     ]
    }
   ],
   "source": [
    "dataset: pd.DataFrame = dataset.apply(pd.to_numeric, errors='ignore').convert_dtypes()\n",
    "# NOTE: stripping makes the test in the next cell fail\n",
    "# dataset['context'] = dataset['context'].str.strip()\n",
    "# dataset['question'] = dataset['question'].str.strip()\n",
    "dataset.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PASSED: answer-text == context[start:end]\n"
     ]
    }
   ],
   "source": [
    "# Simple tests\n",
    "for _, q in dataset.iterrows():\n",
    "    assert q['answer_text'] == q['context'][q['answer_start']:q['answer_end']]\n",
    "print(\"TEST PASSED: answer-text == context[start:end]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2. Data exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99% percentile of context word count: 282\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sentence_lengths = pd.DataFrame(\n",
    "    dataset[['context', 'question']].applymap(str.split).applymap(len).to_numpy(),\n",
    "    columns=['context_word_count', 'question_word_count'],\n",
    ")\n",
    "quantile99 = int(np.quantile(sentence_lengths['context_word_count'], 0.99))\n",
    "\n",
    "ax = sns.histplot(sentence_lengths)\n",
    "ax.axvline(x=quantile99, color='b')\n",
    "print(f\"99% percentile of context word count: {quantile99}\")\n",
    "# Note: after tokenization the numbers may differ but not dramatically"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3. Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(HF_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sequence\n",
      "[CLS] test sequence [SEP]\n"
     ]
    }
   ],
   "source": [
    "s = \"Test sequence\"\n",
    "t = tokenizer(\"Test sequence\")\n",
    "print(s)\n",
    "print(tokenizer.decode(t['input_ids']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/87599 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc874a75018d4885851f491b6dc1828c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers not found: 171\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the context and questions\n",
    "tok = []\n",
    "log_answers_not_found = 0\n",
    "\n",
    "for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "    # Standard tokenizer\n",
    "    t = tokenizer(\n",
    "        row['question'],\n",
    "        row['context'],\n",
    "        max_length=512,\n",
    "        truncation='only_second',\n",
    "        padding='max_length',\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    offset_mapping = np.array(t['offset_mapping'])\n",
    "    token_type_ids = np.array(t['token_type_ids'])\n",
    "\n",
    "    # Get where the answer is located, by looking at tokens that satisfy:\n",
    "    #  - they start after the answer\n",
    "    #  - they end before the answer\n",
    "    #  - they are part of the context\n",
    "    answer_context = (\n",
    "            (offset_mapping[:, 0] >= row['answer_start']) *\n",
    "            (offset_mapping[:, 1] <= row['answer_end']) *\n",
    "            token_type_ids.astype(bool)\n",
    "    )\n",
    "\n",
    "    # Note: for now truncation is not handled\n",
    "    # Debug printing\n",
    "    # print(row['answer_text'])\n",
    "    # print(tokenizer.decode(np.array(t['input_ids'])[answer_context]))\n",
    "    # print(answer_context)\n",
    "\n",
    "    # Get the first and last index of the answer context\n",
    "    answer_tok_idx = np.argwhere(answer_context).ravel()\n",
    "    isp = iep = 0\n",
    "    if answer_tok_idx.size == 0:\n",
    "        log_answers_not_found += 1\n",
    "    else:\n",
    "        isp = answer_tok_idx[0]\n",
    "        iep = answer_tok_idx[-1]\n",
    "        assert isp <= iep\n",
    "\n",
    "    tok.append({\n",
    "        'input_ids': torch.tensor(t['input_ids']),\n",
    "        'token_type_ids': torch.tensor(token_type_ids),\n",
    "        'attention_mask': torch.tensor(t['attention_mask']),\n",
    "        'start_positions': torch.tensor(isp),\n",
    "        'end_positions': torch.tensor(iep),\n",
    "        'offset_mapping': offset_mapping,\n",
    "    })\n",
    "\n",
    "print(f\"Answers not found: {log_answers_not_found}\")\n",
    "# To handle this, we need to do handle larger inputs by slicing or ignore it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                         id                     title  \\\n0  5733be284776f41900661182  University_of_Notre_Dame   \n1  5733be284776f4190066117f  University_of_Notre_Dame   \n2  5733be284776f41900661180  University_of_Notre_Dame   \n3  5733be284776f41900661181  University_of_Notre_Dame   \n4  5733be284776f4190066117e  University_of_Notre_Dame   \n\n                                             context  \\\n0  Architecturally, the school has a Catholic cha...   \n1  Architecturally, the school has a Catholic cha...   \n2  Architecturally, the school has a Catholic cha...   \n3  Architecturally, the school has a Catholic cha...   \n4  Architecturally, the school has a Catholic cha...   \n\n                                            question  \\\n0  To whom did the Virgin Mary allegedly appear i...   \n1  What is in front of the Notre Dame Main Building?   \n2  The Basilica of the Sacred heart at Notre Dame...   \n3                  What is the Grotto at Notre Dame?   \n4  What sits on top of the Main Building at Notre...   \n\n                               answer_text  answer_start  answer_end  \\\n0               Saint Bernadette Soubirous           515         541   \n1                a copper statue of Christ           188         213   \n2                        the Main Building           279         296   \n3  a Marian place of prayer and reflection           381         420   \n4       a golden statue of the Virgin Mary            92         126   \n\n                                           input_ids  \\\n0  [tensor(101), tensor(2000), tensor(3183), tens...   \n1  [tensor(101), tensor(2054), tensor(2003), tens...   \n2  [tensor(101), tensor(1996), tensor(13546), ten...   \n3  [tensor(101), tensor(2054), tensor(2003), tens...   \n4  [tensor(101), tensor(2054), tensor(7719), tens...   \n\n                                      token_type_ids  \\\n0  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n1  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n2  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n3  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n4  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n\n                                      attention_mask start_positions  \\\n0  [tensor(1), tensor(1), tensor(1), tensor(1), t...     tensor(130)   \n1  [tensor(1), tensor(1), tensor(1), tensor(1), t...      tensor(52)   \n2  [tensor(1), tensor(1), tensor(1), tensor(1), t...      tensor(81)   \n3  [tensor(1), tensor(1), tensor(1), tensor(1), t...      tensor(95)   \n4  [tensor(1), tensor(1), tensor(1), tensor(1), t...      tensor(33)   \n\n  end_positions                                     offset_mapping  \n0   tensor(137)  [[0, 0], [0, 2], [3, 7], [8, 11], [12, 15], [1...  \n1    tensor(56)  [[0, 0], [0, 4], [5, 7], [8, 10], [11, 16], [1...  \n2    tensor(83)  [[0, 0], [0, 3], [4, 12], [13, 15], [16, 19], ...  \n3   tensor(101)  [[0, 0], [0, 4], [5, 7], [8, 11], [12, 14], [1...  \n4    tensor(39)  [[0, 0], [0, 4], [5, 9], [10, 12], [13, 16], [...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer_text</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n      <th>start_positions</th>\n      <th>end_positions</th>\n      <th>offset_mapping</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5733be284776f41900661182</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>To whom did the Virgin Mary allegedly appear i...</td>\n      <td>Saint Bernadette Soubirous</td>\n      <td>515</td>\n      <td>541</td>\n      <td>[tensor(101), tensor(2000), tensor(3183), tens...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(130)</td>\n      <td>tensor(137)</td>\n      <td>[[0, 0], [0, 2], [3, 7], [8, 11], [12, 15], [1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5733be284776f4190066117f</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What is in front of the Notre Dame Main Building?</td>\n      <td>a copper statue of Christ</td>\n      <td>188</td>\n      <td>213</td>\n      <td>[tensor(101), tensor(2054), tensor(2003), tens...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(52)</td>\n      <td>tensor(56)</td>\n      <td>[[0, 0], [0, 4], [5, 7], [8, 10], [11, 16], [1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5733be284776f41900661180</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n      <td>the Main Building</td>\n      <td>279</td>\n      <td>296</td>\n      <td>[tensor(101), tensor(1996), tensor(13546), ten...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(81)</td>\n      <td>tensor(83)</td>\n      <td>[[0, 0], [0, 3], [4, 12], [13, 15], [16, 19], ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5733be284776f41900661181</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What is the Grotto at Notre Dame?</td>\n      <td>a Marian place of prayer and reflection</td>\n      <td>381</td>\n      <td>420</td>\n      <td>[tensor(101), tensor(2054), tensor(2003), tens...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(95)</td>\n      <td>tensor(101)</td>\n      <td>[[0, 0], [0, 4], [5, 7], [8, 11], [12, 14], [1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5733be284776f4190066117e</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What sits on top of the Main Building at Notre...</td>\n      <td>a golden statue of the Virgin Mary</td>\n      <td>92</td>\n      <td>126</td>\n      <td>[tensor(101), tensor(2054), tensor(7719), tens...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(33)</td>\n      <td>tensor(39)</td>\n      <td>[[0, 0], [0, 4], [5, 9], [10, 12], [13, 16], [...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = pd.concat((dataset, pd.DataFrame(tok)), axis=1)\n",
    "tokenized_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Test how the questions+context are tokenized and decoded\n",
    "q = tokenized_dataset.iloc[0]\n",
    "s = q['question'] + q['context']\n",
    "t = q['input_ids']\n",
    "print(s)\n",
    "print()\n",
    "print(tokenizer.decode(t))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4. Train-Val split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 78964\n",
      "Validation samples: 8635\n",
      "Actual fraction: 0.9014\n"
     ]
    }
   ],
   "source": [
    "# Get the questions' titles and shuffle them\n",
    "titles = tokenized_dataset['title'].unique()\n",
    "shuffled_titles = pd.Series(titles).sample(frac=1, random_state=RS)\n",
    "\n",
    "# Get the Question Indices grouped by title\n",
    "qi_by_titles = tokenized_dataset.groupby(['title']).indices\n",
    "training_indices = []\n",
    "min_train_len = int(len(tokenized_dataset) * TRAIN_FRACTION)\n",
    "\n",
    "# Add questions until enough are present\n",
    "for title in shuffled_titles:\n",
    "    training_indices += qi_by_titles[title].tolist()\n",
    "    if len(training_indices) >= min_train_len:\n",
    "        break\n",
    "\n",
    "# Create the datasets using the indices\n",
    "ds_train = tokenized_dataset.iloc[training_indices]\n",
    "ds_val = tokenized_dataset.drop(ds_train.index)\n",
    "\n",
    "print(f\"Training samples: {len(ds_train)}\")\n",
    "print(f\"Validation samples: {len(ds_val)}\")\n",
    "print(f\"Actual fraction: {len(ds_train) / len(tokenized_dataset):.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5. Converting the data for PyTorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        return {\n",
    "            'input_ids': item['input_ids'],\n",
    "            'token_type_ids': item['token_type_ids'],\n",
    "            'attention_mask': item['attention_mask'],\n",
    "            'start_positions': item['start_positions'],\n",
    "            'end_positions': item['end_positions']\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(QADataset(ds_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(QADataset(ds_val), batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. Loading the model\n",
    "[docs](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForQuestionAnswering)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "model: BertForQuestionAnswering = BertForQuestionAnswering.from_pretrained(HF_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "================================================================================\nLayer (type:depth-idx)                                  Param #\n================================================================================\nBertForQuestionAnswering                                --\n├─BertModel: 1-1                                        --\n│    └─BertEmbeddings: 2-1                              --\n│    │    └─Embedding: 3-1                              3,906,816\n│    │    └─Embedding: 3-2                              65,536\n│    │    └─Embedding: 3-3                              256\n│    │    └─LayerNorm: 3-4                              256\n│    │    └─Dropout: 3-5                                --\n│    └─BertEncoder: 2-2                                 --\n│    │    └─ModuleList: 3-6                             396,544\n├─Linear: 1-2                                           258\n================================================================================\nTotal params: 4,369,666\nTrainable params: 4,369,666\nNon-trainable params: 0\n================================================================================"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "def calc_f1(answer_text, answer_text_pred):\n",
    "    f1s = []\n",
    "    for text, pred in zip(answer_text, answer_text_pred):\n",
    "        f1s.append(evaluate.compute_f1(text, pred))\n",
    "    return pd.Series(f1s, name='f1')\n",
    "    # return evaluate.compute_f1(answer_text, answer_text_pred)\n",
    "\n",
    "\n",
    "def calc_em_str(answer_text, answer_text_pred):\n",
    "    # Use string comparison\n",
    "    return (answer_text == answer_text_pred).astype(int).rename('em')\n",
    "\n",
    "\n",
    "def calc_em_tensor(answer_start, answer_end, answer_start_pred, answer_end_pred):\n",
    "    # Use TOKEN index comparison\n",
    "    return torch.logical_and(\n",
    "        answer_start == answer_start_pred,\n",
    "        answer_end == answer_end_pred,\n",
    "    ).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3. Validation / Evaluation function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader: DataLoader, return_frame=False, display_bar=False):\n",
    "    _loader = tqdm(loader) if display_bar else loader\n",
    "\n",
    "    # Store the answers' TOK indices\n",
    "    answer_start_tok= []\n",
    "    answer_end_tok=[]\n",
    "\n",
    "\n",
    "    # Get outputs from the model\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in _loader:\n",
    "            args = dict(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                token_type_ids=batch['token_type_ids'].to(device),\n",
    "            )\n",
    "            outputs = model(**args)\n",
    "\n",
    "            # Get the answers' TOK indices\n",
    "            answer_start_tok += torch.argmax(outputs['start_logits'], dim=1).tolist()\n",
    "            answer_end_tok += torch.argmax(outputs['end_logits'], dim=1).tolist()\n",
    "\n",
    "    # Convert the TOK indices into TEXT using the context\n",
    "    df = loader.dataset.df.reset_index()\n",
    "    answer_text_pred = []\n",
    "    for idx, row in df.iterrows():\n",
    "        om = row['offset_mapping']\n",
    "        tst, ten = answer_start_tok[idx], answer_end_tok[idx]  # Tok start, tok end\n",
    "        cst, cen = om[tst, 0], om[ten, 1]  # Char start, Char end\n",
    "        answer_text_pred.append(row['context'][cst:cen])\n",
    "    answer_text_pred = pd.Series(answer_text_pred)\n",
    "\n",
    "    # Compute the metrics\n",
    "    f1s = calc_f1(df['answer_text'], answer_text_pred)\n",
    "    ems = calc_em_str(df['answer_text'], answer_text_pred)\n",
    "\n",
    "    out = {'f1_mean': sum(f1s)/len(f1s), 'em_mean': sum(ems)/len(ems)}\n",
    "    if return_frame:\n",
    "        out['dataframe'] = pd.concat([df, pd.DataFrame({'answer_text_pred':answer_text_pred, 'f1': f1s, 'em':ems})], axis=1)\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Wandb Integration & Model save-load functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT=True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging run quiet-morning-4 at https://wandb.ai/frantoman/NLP-Question-Answering/runs/2995rh9v\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb_params = dict(\n",
    "    project=\"NLP-Question-Answering\",\n",
    "    entity=\"frantoman\",\n",
    "    reinit=True,\n",
    "    group=HF_MODEL_NAME,\n",
    "    config=dict(\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        optimizer='adam',\n",
    "        model_name=HF_MODEL_NAME,\n",
    "    )\n",
    ")\n",
    "\n",
    "run = None\n",
    "if USE_WANDB:\n",
    "    run = wandb.init(project=\"NLP-Question-Answering\", entity=\"frantoman\")\n",
    "    print(f\"Logging run {run.name} at {run.url}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "Path(MODELS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "save_filepath = f\"{MODELS_FOLDER}/{MODEL_SAVE_NAME}_{datetime.today().strftime('%m%d')}.pt\"\n",
    "\n",
    "def save_model(model, filepath=save_filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    # print(f'Model saved in {filepath}')\n",
    "\n",
    "def load_model(model, filepath=save_filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    print(f'Loaded model at {filepath}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5. Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/4936 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2613e97fc06d40d5a6e5d5892d605bba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./models/bert-tiny_0131.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2:   0%|          | 0/4936 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75bdde90e41141f28014cfd43fc5e3b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./models/bert-tiny_0131.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3:   0%|          | 0/4936 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56e953d5ad254a96ab868a8c6d835642"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./models/bert-tiny_0131.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 4:   0%|          | 0/4936 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa777710bc3444e4b57cf5c47fcb6bcc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Epoch 5:   0%|          | 0/4936 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d73591aa8af4a7a84c91e06de00d50a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize stuff\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "torch.cuda.empty_cache()\n",
    "model.train()\n",
    "best_f1 = 0  # Used for determining when the model performance over the epochs is degrading\n",
    "\n",
    "# Iterate through the epochs\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Epoch's history\n",
    "    ep_loss = []\n",
    "    ep_em = []\n",
    "\n",
    "    train_iter = tqdm(train_loader, desc=f'Epoch {epoch}', leave=True)\n",
    "\n",
    "    # Training\n",
    "    for train_batch in train_iter:\n",
    "        # Extract the model arguments from the batch, and do a forward-pass\n",
    "        opt.zero_grad()\n",
    "        args = dict(\n",
    "            input_ids=train_batch['input_ids'].to(device),\n",
    "            attention_mask=train_batch['attention_mask'].to(device),\n",
    "            start_positions=train_batch['start_positions'].to(device),\n",
    "            end_positions=train_batch['end_positions'].to(device),\n",
    "            token_type_ids=train_batch['token_type_ids'].to(device),\n",
    "        )\n",
    "        outputs = model(**args)\n",
    "\n",
    "        # Get the starting and end token indices\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        # Log the Loss and Exact Matches for the batch\n",
    "        loss = outputs['loss']\n",
    "        em = calc_em_tensor(args['start_positions'], args['end_positions'], start_pred, end_pred)\n",
    "        em_mean = em.mean().item()\n",
    "        ep_loss.append(loss.item())\n",
    "        ep_em.append(em_mean)\n",
    "\n",
    "        # Update the progress bar\n",
    "        train_iter.set_postfix(loss=sum(ep_loss[-50:]) / len(ep_loss[-50:]),\n",
    "                               em=sum(ep_em[-50:]) / len(ep_em[-50:]))\n",
    "\n",
    "        # Backwards-pass\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # Validation & Saving\n",
    "    val_out = evaluate_model(model, val_loader)\n",
    "    f1 = val_out['f1_mean']\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        save_model(model)\n",
    "\n",
    "    # End of epoch\n",
    "    train_iter.close()\n",
    "    if USE_WANDB:\n",
    "        run.log(dict(\n",
    "            epoch=epoch,\n",
    "            em=sum(ep_em) / len(ep_em),\n",
    "            loss=sum(ep_loss) / len(ep_loss),\n",
    "            val_em=val_out['em_mean'],\n",
    "            val_f1=val_out['f1_mean'],\n",
    "        ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6. Uploading artifacts to Wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at ./models/bert-tiny_0131.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": "<wandb.sdk.wandb_artifacts.Artifact at 0x1cc8fd796a0>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the fine-tuned model\n",
    "model_save_artifact = wandb.Artifact('model', type='model')\n",
    "model_save_artifact.add_file(save_filepath)\n",
    "run.log_artifact(model_save_artifact)\n",
    "\n",
    "# Upload some (100) validation outputs of the model\n",
    "load_model(model)\n",
    "out = evaluate_model(model, DataLoader(QADataset(ds_val.iloc[:100])), return_frame=True)\n",
    "df = out['dataframe'][['question', 'context', 'answer_text', 'answer_text_pred', 'f1', 'em']]\n",
    "out_table = wandb.Table(data=df, columns=df.columns)\n",
    "result_artifact = wandb.Artifact('validation_output', type='result')\n",
    "result_artifact.add(out_table, 'validation_output')\n",
    "run.log_artifact(result_artifact)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def output_to_file(data: pd.DataFrame):\n",
    "    raw_out = {}\n",
    "    data.apply(lambda row: raw_out.update({row['id']: row['answer_text_pred']}), axis=1)\n",
    "    with open('predictions.txt', 'w+') as fout:\n",
    "        fout.write(json.dumps(raw_out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %run evaluate.py \"training_set.json\" \"predictions.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}