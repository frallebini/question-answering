{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Config & Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# HF_MODEL_NAME = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "HF_MODEL_NAME = \"prajjwal1/bert-medium\"\n",
    "MODELS_FOLDER = './models'\n",
    "MODEL_SAVE_NAME = \"bert-medium-final\"\n",
    "MODEL_LOAD_NAME = None\n",
    "USE_WANDB = False\n",
    "USE_CUSTOM_MODEL = False\n",
    "CHECK_VAL_BOUNDS = True\n",
    "\n",
    "RS = 42  # Random state\n",
    "DROPOUT = 0.0\n",
    "EPOCHS = 5\n",
    "TRAIN_FRACTION = 0.9\n",
    "BATCH_SIZE = 16\n",
    "BASE_LEARNING_RATE = 1e-5\n",
    "MAX_LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import torchinfo\n",
    "import transformers\n",
    "from transformers import BertModel, BertForQuestionAnswering, AutoTokenizer, PreTrainedTokenizerFast\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1. Json to Dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "                         id                     title  \\\n0  5733be284776f41900661182  University_of_Notre_Dame   \n1  5733be284776f4190066117f  University_of_Notre_Dame   \n2  5733be284776f41900661180  University_of_Notre_Dame   \n3  5733be284776f41900661181  University_of_Notre_Dame   \n4  5733be284776f4190066117e  University_of_Notre_Dame   \n\n                                             context  \\\n0  Architecturally, the school has a Catholic cha...   \n1  Architecturally, the school has a Catholic cha...   \n2  Architecturally, the school has a Catholic cha...   \n3  Architecturally, the school has a Catholic cha...   \n4  Architecturally, the school has a Catholic cha...   \n\n                                            question  \\\n0  To whom did the Virgin Mary allegedly appear i...   \n1  What is in front of the Notre Dame Main Building?   \n2  The Basilica of the Sacred heart at Notre Dame...   \n3                  What is the Grotto at Notre Dame?   \n4  What sits on top of the Main Building at Notre...   \n\n                               answer_text  answer_start  answer_end  \n0               Saint Bernadette Soubirous           515         541  \n1                a copper statue of Christ           188         213  \n2                        the Main Building           279         296  \n3  a Marian place of prayer and reflection           381         420  \n4       a golden statue of the Virgin Mary            92         126  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer_text</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5733be284776f41900661182</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>To whom did the Virgin Mary allegedly appear i...</td>\n      <td>Saint Bernadette Soubirous</td>\n      <td>515</td>\n      <td>541</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5733be284776f4190066117f</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What is in front of the Notre Dame Main Building?</td>\n      <td>a copper statue of Christ</td>\n      <td>188</td>\n      <td>213</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5733be284776f41900661180</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n      <td>the Main Building</td>\n      <td>279</td>\n      <td>296</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5733be284776f41900661181</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What is the Grotto at Notre Dame?</td>\n      <td>a Marian place of prayer and reflection</td>\n      <td>381</td>\n      <td>420</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5733be284776f4190066117e</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What sits on top of the Main Building at Notre...</td>\n      <td>a golden statue of the Virgin Mary</td>\n      <td>92</td>\n      <td>126</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the raw json\n",
    "url = 'training_set.json'\n",
    "with open(url, 'r') as file:\n",
    "    raw = json.load(file)['data']\n",
    "data = []\n",
    "for topic in raw:\n",
    "    for paragraph in topic['paragraphs']:\n",
    "        for question in paragraph['qas']:\n",
    "            assert len(question['answers']) == 1\n",
    "            answer = question['answers'][0]\n",
    "            data.append((\n",
    "                question['id'],\n",
    "                topic['title'],\n",
    "                paragraph['context'],\n",
    "                question['question'],\n",
    "                answer['text'],\n",
    "                answer['answer_start'],\n",
    "                answer['answer_start'] + len(answer['text']),\n",
    "            ))\n",
    "dataset = pd.DataFrame(data,\n",
    "                       columns=('id', 'title', 'context', 'question', 'answer_text', 'answer_start', 'answer_end'))\n",
    "dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# Convert the data to string or integer\n",
    "dataset: pd.DataFrame = dataset.apply(pd.to_numeric, errors='ignore').convert_dtypes()\n",
    "# NOTE: stripping makes the test in the next cell fail\n",
    "# dataset['context'] = dataset['context'].str.strip()\n",
    "# dataset['question'] = dataset['question'].str.strip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PASSED: answer-text == context[start:end]\n"
     ]
    }
   ],
   "source": [
    "# Simple tests\n",
    "for _, q in dataset.iterrows():\n",
    "    assert q['answer_text'] == q['context'][q['answer_start']:q['answer_end']]\n",
    "print(\"TEST PASSED: answer-text == context[start:end]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2. Data exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99% percentile of question + context word count: 303\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sentence_lengths = pd.DataFrame(\n",
    "    dataset[['context', 'question']].applymap(str.split).applymap(len).to_numpy(),\n",
    "    columns=['context_word_count', 'question_word_count'],\n",
    ")\n",
    "p99_context = int(np.quantile(sentence_lengths['context_word_count'], 0.99))\n",
    "p99_question = int(np.quantile(sentence_lengths['question_word_count'], 0.99))\n",
    "percentile99 = p99_question + p99_context\n",
    "\n",
    "ax = sns.histplot(sentence_lengths)\n",
    "ax.axvline(x=percentile99, color='b')\n",
    "print(f\"99% percentile of question + context word count: {percentile99}\")\n",
    "# Note: after tokenization the numbers may differ but not dramatically"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3. Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/87599 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f645ccf66b7a465985c9e0541f072d1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers not found due to truncation: 171\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the context and questions\n",
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "tok = []\n",
    "log_answers_not_found = 0\n",
    "\n",
    "for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "    # Standard tokenizer\n",
    "    t = tokenizer(\n",
    "        row['question'],\n",
    "        row['context'],\n",
    "        max_length=512,\n",
    "        truncation='only_second',\n",
    "        padding='max_length',\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    offset_mapping = np.array(t['offset_mapping'])\n",
    "    token_type_ids = np.array(t['token_type_ids'])\n",
    "\n",
    "    # Get where the answer is located, by looking at tokens that satisfy:\n",
    "    #  - they start after the answer\n",
    "    #  - they end before the answer\n",
    "    #  - they are part of the context\n",
    "    answer_context = (\n",
    "            (offset_mapping[:, 0] >= row['answer_start']) *\n",
    "            (offset_mapping[:, 1] <= row['answer_end']) *\n",
    "            token_type_ids.astype(bool)\n",
    "    )\n",
    "\n",
    "    # Note: for now truncation is not handled\n",
    "    # Debug printing\n",
    "    # print(row['answer_text'])\n",
    "    # print(tokenizer.decode(np.array(t['input_ids'])[answer_context]))\n",
    "    # print(answer_context)\n",
    "\n",
    "    # Get the first and last index of the answer context\n",
    "    answer_tok_idx = np.argwhere(answer_context).ravel()\n",
    "    isp = iep = 0\n",
    "    if answer_tok_idx.size == 0:\n",
    "        log_answers_not_found += 1\n",
    "    else:\n",
    "        isp = answer_tok_idx[0]\n",
    "        iep = answer_tok_idx[-1]\n",
    "        assert isp <= iep\n",
    "\n",
    "    tok.append({\n",
    "        'input_ids': torch.tensor(t['input_ids']),\n",
    "        'token_type_ids': torch.tensor(token_type_ids),\n",
    "        'attention_mask': torch.tensor(t['attention_mask']),\n",
    "        'start_positions': torch.tensor(isp),\n",
    "        'end_positions': torch.tensor(iep),\n",
    "        'offset_mapping': offset_mapping,\n",
    "    })\n",
    "\n",
    "print(f\"Answers not found due to truncation: {log_answers_not_found}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "                         id                     title  \\\n0  5733be284776f41900661182  University_of_Notre_Dame   \n1  5733be284776f4190066117f  University_of_Notre_Dame   \n2  5733be284776f41900661180  University_of_Notre_Dame   \n3  5733be284776f41900661181  University_of_Notre_Dame   \n4  5733be284776f4190066117e  University_of_Notre_Dame   \n\n                                             context  \\\n0  Architecturally, the school has a Catholic cha...   \n1  Architecturally, the school has a Catholic cha...   \n2  Architecturally, the school has a Catholic cha...   \n3  Architecturally, the school has a Catholic cha...   \n4  Architecturally, the school has a Catholic cha...   \n\n                                            question  \\\n0  To whom did the Virgin Mary allegedly appear i...   \n1  What is in front of the Notre Dame Main Building?   \n2  The Basilica of the Sacred heart at Notre Dame...   \n3                  What is the Grotto at Notre Dame?   \n4  What sits on top of the Main Building at Notre...   \n\n                               answer_text  answer_start  answer_end  \\\n0               Saint Bernadette Soubirous           515         541   \n1                a copper statue of Christ           188         213   \n2                        the Main Building           279         296   \n3  a Marian place of prayer and reflection           381         420   \n4       a golden statue of the Virgin Mary            92         126   \n\n                                           input_ids  \\\n0  [tensor(101), tensor(2000), tensor(3183), tens...   \n1  [tensor(101), tensor(2054), tensor(2003), tens...   \n2  [tensor(101), tensor(1996), tensor(13546), ten...   \n3  [tensor(101), tensor(2054), tensor(2003), tens...   \n4  [tensor(101), tensor(2054), tensor(7719), tens...   \n\n                                      token_type_ids  \\\n0  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n1  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n2  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n3  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n4  [tensor(0, dtype=torch.int32), tensor(0, dtype...   \n\n                                      attention_mask start_positions  \\\n0  [tensor(1), tensor(1), tensor(1), tensor(1), t...     tensor(130)   \n1  [tensor(1), tensor(1), tensor(1), tensor(1), t...      tensor(52)   \n2  [tensor(1), tensor(1), tensor(1), tensor(1), t...      tensor(81)   \n3  [tensor(1), tensor(1), tensor(1), tensor(1), t...      tensor(95)   \n4  [tensor(1), tensor(1), tensor(1), tensor(1), t...      tensor(33)   \n\n  end_positions                                     offset_mapping  \n0   tensor(137)  [[0, 0], [0, 2], [3, 7], [8, 11], [12, 15], [1...  \n1    tensor(56)  [[0, 0], [0, 4], [5, 7], [8, 10], [11, 16], [1...  \n2    tensor(83)  [[0, 0], [0, 3], [4, 12], [13, 15], [16, 19], ...  \n3   tensor(101)  [[0, 0], [0, 4], [5, 7], [8, 11], [12, 14], [1...  \n4    tensor(39)  [[0, 0], [0, 4], [5, 9], [10, 12], [13, 16], [...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer_text</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n      <th>start_positions</th>\n      <th>end_positions</th>\n      <th>offset_mapping</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5733be284776f41900661182</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>To whom did the Virgin Mary allegedly appear i...</td>\n      <td>Saint Bernadette Soubirous</td>\n      <td>515</td>\n      <td>541</td>\n      <td>[tensor(101), tensor(2000), tensor(3183), tens...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(130)</td>\n      <td>tensor(137)</td>\n      <td>[[0, 0], [0, 2], [3, 7], [8, 11], [12, 15], [1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5733be284776f4190066117f</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What is in front of the Notre Dame Main Building?</td>\n      <td>a copper statue of Christ</td>\n      <td>188</td>\n      <td>213</td>\n      <td>[tensor(101), tensor(2054), tensor(2003), tens...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(52)</td>\n      <td>tensor(56)</td>\n      <td>[[0, 0], [0, 4], [5, 7], [8, 10], [11, 16], [1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5733be284776f41900661180</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n      <td>the Main Building</td>\n      <td>279</td>\n      <td>296</td>\n      <td>[tensor(101), tensor(1996), tensor(13546), ten...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(81)</td>\n      <td>tensor(83)</td>\n      <td>[[0, 0], [0, 3], [4, 12], [13, 15], [16, 19], ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5733be284776f41900661181</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What is the Grotto at Notre Dame?</td>\n      <td>a Marian place of prayer and reflection</td>\n      <td>381</td>\n      <td>420</td>\n      <td>[tensor(101), tensor(2054), tensor(2003), tens...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(95)</td>\n      <td>tensor(101)</td>\n      <td>[[0, 0], [0, 4], [5, 7], [8, 11], [12, 14], [1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5733be284776f4190066117e</td>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n      <td>What sits on top of the Main Building at Notre...</td>\n      <td>a golden statue of the Virgin Mary</td>\n      <td>92</td>\n      <td>126</td>\n      <td>[tensor(101), tensor(2054), tensor(7719), tens...</td>\n      <td>[tensor(0, dtype=torch.int32), tensor(0, dtype...</td>\n      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n      <td>tensor(33)</td>\n      <td>tensor(39)</td>\n      <td>[[0, 0], [0, 4], [5, 9], [10, 12], [13, 16], [...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = pd.concat((dataset, pd.DataFrame(tok)), axis=1)\n",
    "tokenized_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Test how the questions+context are tokenized and decoded\n",
    "q = tokenized_dataset.iloc[0]\n",
    "s = q['question'] + q['context']\n",
    "t = q['input_ids']\n",
    "print(s)\n",
    "print()\n",
    "print(tokenizer.decode(t))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4. Train-Val split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 78964\n",
      "Validation samples: 8635\n",
      "Actual fraction: 0.9014\n"
     ]
    }
   ],
   "source": [
    "# Get the questions' titles and shuffle them\n",
    "titles = tokenized_dataset['title'].unique()\n",
    "shuffled_titles = pd.Series(titles).sample(frac=1, random_state=RS)\n",
    "\n",
    "# Get the Question Indices grouped by title\n",
    "qi_by_titles = tokenized_dataset.groupby(['title']).indices\n",
    "training_indices = []\n",
    "min_train_len = int(len(tokenized_dataset) * TRAIN_FRACTION)\n",
    "\n",
    "# Add questions until enough are present\n",
    "for title in shuffled_titles:\n",
    "    training_indices += qi_by_titles[title].tolist()\n",
    "    if len(training_indices) >= min_train_len:\n",
    "        break\n",
    "\n",
    "# Create the datasets using the indices\n",
    "ds_train = tokenized_dataset.iloc[training_indices]\n",
    "ds_val = tokenized_dataset.drop(ds_train.index)\n",
    "\n",
    "print(f\"Training samples: {len(ds_train)}\")\n",
    "print(f\"Validation samples: {len(ds_val)}\")\n",
    "print(f\"Actual fraction: {len(ds_train) / len(tokenized_dataset):.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5. Converting the data for PyTorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        return {\n",
    "            'input_ids': item['input_ids'],\n",
    "            'token_type_ids': item['token_type_ids'],\n",
    "            'attention_mask': item['attention_mask'],\n",
    "            'start_positions': item['start_positions'],\n",
    "            'end_positions': item['end_positions']\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def load(ds, shuffle,batch_size=BATCH_SIZE):\n",
    "        return DataLoader(QADataset(ds), batch_size=batch_size, shuffle=shuffle, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "train_loader = QADataset.load(ds_train, shuffle=True)\n",
    "val_loader = QADataset.load(ds_val, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. Loading the model & Custom model definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/bert/modeling_bert.py#L1792\n",
    "class CustomBertForQuestionAnswering(BertForQuestionAnswering):\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnswering, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = nn.Sequential(\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(config.hidden_size, config.num_labels),\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if USE_CUSTOM_MODEL:\n",
    "    model: BertForQuestionAnswering = CustomBertForQuestionAnswering.from_pretrained(HF_MODEL_NAME)\n",
    "    print(\"Using custom model\")\n",
    "else:\n",
    "    model: BertForQuestionAnswering = BertForQuestionAnswering.from_pretrained(HF_MODEL_NAME)\n",
    "    print(\"Using Huggingface Question Answering model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Huggingface Question Answering model\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "=========================================================================================================\nLayer (type:depth-idx)                                  Output Shape              Param #\n=========================================================================================================\nBertForQuestionAnswering                                --                        --\n├─BertModel: 1                                          --                        --\n│    └─BertEncoder: 2                                   --                        --\n│    │    └─ModuleList: 3-1                             --                        25,219,072\n├─BertModel: 1-1                                        [16, 512, 512]            --\n│    └─BertEmbeddings: 2-1                              [16, 512, 512]            --\n│    │    └─Embedding: 3-2                              [16, 512, 512]            15,627,264\n│    │    └─Embedding: 3-3                              [16, 512, 512]            1,024\n│    │    └─Embedding: 3-4                              [1, 512, 512]             262,144\n│    │    └─LayerNorm: 3-5                              [16, 512, 512]            1,024\n│    │    └─Dropout: 3-6                                [16, 512, 512]            --\n│    └─BertEncoder: 2-2                                 [16, 512, 512]            --\n├─Linear: 1-2                                           [16, 512, 2]              1,026\n=========================================================================================================\nTotal params: 41,111,554\nTrainable params: 41,111,554\nNon-trainable params: 0\nTotal mult-adds (M): 653.85\n=========================================================================================================\nInput size (MB): 0.16\nForward/backward pass size (MB): 3055.68\nParams size (MB): 164.45\nEstimated Total Size (MB): 3220.29\n========================================================================================================="
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_data=next(iter(QADataset.load(ds_val, shuffle=False))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def calc_f1(answer_text, answer_text_pred):\n",
    "    f1s = []\n",
    "    for text, pred in zip(answer_text, answer_text_pred):\n",
    "        f1s.append(evaluate.compute_f1(text, pred))\n",
    "    return pd.Series(f1s, name='f1')\n",
    "\n",
    "def calc_em_str(answer_text, answer_text_pred):\n",
    "    # Use string comparison\n",
    "    return (answer_text == answer_text_pred).astype(int).rename('em')\n",
    "\n",
    "def calc_em_tensor(answer_start, answer_end, answer_start_pred, answer_end_pred):\n",
    "    # Use TOKEN index comparison\n",
    "    return torch.logical_and(\n",
    "        answer_start == answer_start_pred,\n",
    "        answer_end == answer_end_pred,\n",
    "    ).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3. Validation / Evaluation function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader: DataLoader, check_bounds, return_frame=False, display_bar=False):\n",
    "    _loader = tqdm(loader) if display_bar else loader\n",
    "\n",
    "    # Store the answers' TOK indices\n",
    "    answer_start_tok= []\n",
    "    answer_end_tok= []\n",
    "\n",
    "    # Get outputs from the model\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in _loader:\n",
    "            args = dict(\n",
    "                input_ids=batch['input_ids'].to(device, non_blocking=True),\n",
    "                attention_mask=batch['attention_mask'].to(device, non_blocking=True),\n",
    "                token_type_ids=batch['token_type_ids'].to(device, non_blocking=True),\n",
    "            )\n",
    "            outputs = model(**args)\n",
    "\n",
    "            # Get the answers' TOK indices, masked by the Token-Type-Ids (1s only iff context)\n",
    "            mask = ((args['token_type_ids'] - 1) * torch.inf).nan_to_num(0)  # -inf if tok!=context; 0 otherwise\n",
    "            start_logits = outputs['start_logits'] + mask\n",
    "            end_logits = outputs['end_logits'] + mask\n",
    "            start_tok_indices = torch.argmax(start_logits, dim=1).tolist()\n",
    "            end_tok_indices = torch.argmax(end_logits, dim=1).tolist()\n",
    "\n",
    "            if not check_bounds:\n",
    "                answer_start_tok += start_tok_indices\n",
    "                answer_end_tok += end_tok_indices\n",
    "                continue\n",
    "\n",
    "            for k in range(len(start_tok_indices)):\n",
    "                sti, eti = start_tok_indices[k], end_tok_indices[k]\n",
    "                # Check if start token index <= end token index (VALID)\n",
    "                if sti <= eti:\n",
    "                    answer_start_tok.append(sti)\n",
    "                    answer_end_tok.append(eti)\n",
    "                    continue\n",
    "\n",
    "                # Otherwise, pick (start, end) = (i, j)\n",
    "                # such that i <= j and start_logits[i] + end_logits[j] is the highest\n",
    "                # among all the (i, j) couples\n",
    "                valid_logits = {}\n",
    "                candidate_sti = start_logits[k].argsort(descending=True)[:20]\n",
    "                candidate_eti = end_logits[k].argsort(descending=True)[:20]\n",
    "\n",
    "                for i in candidate_sti:\n",
    "                    for j in candidate_eti:\n",
    "                        if i <= j:\n",
    "                            valid_logits[(i, j)] = (start_logits[k, i] + end_logits[k, j]).item()\n",
    "\n",
    "                # Get the argmax of the valid logits sum and append the results\n",
    "                if len(valid_logits) > 0:\n",
    "                    sti, eti = max(valid_logits, key=valid_logits.get)\n",
    "                answer_start_tok.append(sti)\n",
    "                answer_end_tok.append(eti)\n",
    "\n",
    "    # Convert the TOK indices into TEXT using the context\n",
    "    df = loader.dataset.df.reset_index()\n",
    "    answer_text_pred = []\n",
    "    for idx, row in df.iterrows():\n",
    "        om = row['offset_mapping']\n",
    "        tst, ten = answer_start_tok[idx], answer_end_tok[idx]  # Tok start, tok end\n",
    "        cst, cen = om[tst, 0], om[ten, 1]  # Char start, Char end\n",
    "        answer_text_pred.append(row['context'][cst:cen])\n",
    "    answer_text_pred = pd.Series(answer_text_pred)\n",
    "\n",
    "    # Compute the metrics\n",
    "    f1s = calc_f1(df['answer_text'], answer_text_pred)\n",
    "    ems = calc_em_str(df['answer_text'], answer_text_pred)\n",
    "\n",
    "    out = {'f1_mean': sum(f1s)/len(f1s), 'em_mean': sum(ems)/len(ems)}\n",
    "    if return_frame:\n",
    "        out['dataframe'] = pd.concat([df, pd.DataFrame({'answer_text_pred': answer_text_pred,\n",
    "                                                        'f1': f1s,\n",
    "                                                        'em': ems})], axis=1)\n",
    "\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4. Wandb Integration & Model save-load functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "Path(MODELS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "model_save_name = MODEL_SAVE_NAME if MODEL_SAVE_NAME is not None else HF_MODEL_NAME.split('/')[-1]\n",
    "save_filepath = f\"{MODELS_FOLDER}/{model_save_name}_{datetime.today().strftime('%m%d')}.pt\"\n",
    "\n",
    "def save_model(model, filepath=save_filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    # print(f'Model saved in {filepath}')\n",
    "\n",
    "def load_model(model, filepath=save_filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    print(f'Loaded model at {filepath}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT=True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb_params = dict(\n",
    "    project=\"NLP-Question-Answering\",\n",
    "    entity=\"frantoman\",\n",
    "    reinit=True,\n",
    "    group=HF_MODEL_NAME,\n",
    "    name=model_save_name,\n",
    "    config=dict(\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate_base=BASE_LEARNING_RATE,\n",
    "        learning_rate_max=MAX_LEARNING_RATE,\n",
    "        optimizer='adam',\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        model_name=HF_MODEL_NAME,\n",
    "        dropout=DROPOUT,\n",
    "    )\n",
    ")\n",
    "\n",
    "run = None\n",
    "if USE_WANDB:\n",
    "    run = wandb.init(**wandb_params)\n",
    "    print(f\"Logging run {run.name} at {run.url}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5. Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Freeze the bert embedding layers\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize training stuff\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=BASE_LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    opt,\n",
    "    base_lr=BASE_LEARNING_RATE,\n",
    "    max_lr=MAX_LEARNING_RATE,\n",
    "    mode='triangular2',\n",
    "    step_size_up=len(train_loader) // 2,\n",
    "    cycle_momentum=False,\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.train()\n",
    "best_score = 0  # Used for determining when the model performance over the epochs is degrading\n",
    "\n",
    "# Iterate through the epochs\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Epoch's history\n",
    "    ep_loss = []\n",
    "    ep_em = []\n",
    "\n",
    "    train_iter = tqdm(train_loader, desc=f'Epoch {epoch}', leave=True)\n",
    "\n",
    "    # Training\n",
    "    for train_batch in train_iter:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Extract the model arguments from the batch, and do a forward-pass\n",
    "            args = dict(\n",
    "                input_ids=train_batch['input_ids'].to(device, non_blocking=True),\n",
    "                attention_mask=train_batch['attention_mask'].to(device, non_blocking=True),\n",
    "                start_positions=train_batch['start_positions'].to(device, non_blocking=True),\n",
    "                end_positions=train_batch['end_positions'].to(device, non_blocking=True),\n",
    "                token_type_ids=train_batch['token_type_ids'].to(device, non_blocking=True),\n",
    "            )\n",
    "            outputs = model(**args)\n",
    "\n",
    "        # Get the starting and end token indices\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        # Log the Loss and Exact Matches for the batch\n",
    "        loss = outputs['loss']\n",
    "        em = calc_em_tensor(args['start_positions'], args['end_positions'], start_pred, end_pred)\n",
    "        em_mean = em.mean().item()\n",
    "        ep_loss.append(loss.item())\n",
    "        ep_em.append(em_mean)\n",
    "\n",
    "        # Update the progress bar\n",
    "        train_iter.set_postfix(loss=sum(ep_loss[-50:]) / len(ep_loss[-50:]),\n",
    "                               em=sum(ep_em[-50:]) / len(ep_em[-50:]))\n",
    "\n",
    "        # Backwards-pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Validation & Saving\n",
    "    val_out = evaluate_model(model, val_loader, check_bounds=CHECK_VAL_BOUNDS)\n",
    "    f1, em = val_out['f1_mean'], val_out['em_mean']\n",
    "    try:\n",
    "        score = (2 * f1 * em) / (f1 + em)  # Harmonic mean\n",
    "    except ZeroDivisionError:\n",
    "        score = 0\n",
    "    if f1 > best_score:\n",
    "        best_score = score\n",
    "        save_model(model)\n",
    "\n",
    "    # End of epoch logging\n",
    "    train_iter.close()\n",
    "    if USE_WANDB:\n",
    "        run.log(dict(\n",
    "            epoch=epoch,\n",
    "            em=sum(ep_em) / len(ep_em),\n",
    "            loss=sum(ep_loss) / len(ep_loss),\n",
    "            val_em=em,\n",
    "            val_f1=f1,\n",
    "            score=score,\n",
    "        ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6. Uploading artifacts to Wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if USE_WANDB:\n",
    "    # Upload the fine-tuned model\n",
    "    model_save_artifact = wandb.Artifact('model', type='model')\n",
    "    model_save_artifact.add_file(save_filepath)\n",
    "    run.log_artifact(model_save_artifact)\n",
    "\n",
    "    # Upload some (100) validation outputs of the model\n",
    "    load_model(model)\n",
    "    out = evaluate_model(model, QADataset.load(ds_val.iloc[:100], shuffle=False), return_frame=True, check_bounds=True)\n",
    "    df = out['dataframe'][['question', 'context', 'answer_text', 'answer_text_pred', 'f1', 'em']]\n",
    "    out_table = wandb.Table(data=df, columns=df.columns)\n",
    "    result_artifact = wandb.Artifact('validation_output', type='result')\n",
    "    result_artifact.add(out_table, 'validation_output')\n",
    "    run.log_artifact(result_artifact)\n",
    "    run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}