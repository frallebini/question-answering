{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from config import conf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "from dataset import SquadDataset\n",
    "from torch.nn import Module\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "dataset = SquadDataset.from_json(conf['DATASET_FILE'], tokenizer)\n",
    "train_dataset, val_dataset = dataset.train_val_split(conf['TRAIN_RATIO'])\n",
    "\n",
    "model: Module = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(pred: torch.Tensor, true: torch.Tensor) -> float:\n",
    "    assert len(pred) == len(true)\n",
    "    return ((pred == true).sum() / len(pred)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5475/5475 [21:10<00:00,  4.31it/s, acc=0.833, loss=0.722]\n",
      "Epoch 1: 100%|██████████| 5475/5475 [20:50<00:00,  4.38it/s, acc=0.5, loss=1.49]   \n",
      "Epoch 2: 100%|██████████| 5475/5475 [20:56<00:00,  4.36it/s, acc=0.75, loss=0.533] \n",
      "Epoch 3: 100%|██████████| 5475/5475 [20:55<00:00,  4.36it/s, acc=0.667, loss=0.524] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./models/model_0125.pt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "if conf['TRAIN_MODEL']:\n",
    "    \n",
    "    opt = Adam(model.parameters(), lr=5e-5)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=conf['BATCH_SIZE'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=conf['BATCH_SIZE'])\n",
    "\n",
    "    for epoch in range(conf['N_EPOCHS']):\n",
    "        # ================================ TRAINING ============================\n",
    "        model.train()\n",
    "\n",
    "        n_train = 0\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        train_iter = tqdm(train_loader, desc=f'Epoch {epoch}', leave=False)\n",
    "        \n",
    "        for train_batch in train_iter:\n",
    "            input_ids = train_batch['input_ids'].to(device)\n",
    "            attention_mask = train_batch['attention_mask'].to(device)\n",
    "            start_true = train_batch['start_positions'].to(device)\n",
    "            end_true = train_batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            start_positions=start_true,\n",
    "                            end_positions=end_true)\n",
    "            start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "            end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "            assert len(start_true) == len(start_pred) == len(end_true) == len(end_pred)\n",
    "            n_samples = len(start_true)\n",
    "\n",
    "            loss = outputs['loss']  # computed as (start_loss + end_loss) / 2\n",
    "            # where start_loss and end_loss are, in turn, computed as averages over the batch\n",
    "            start_acc = compute_accuracy(start_pred, start_true)\n",
    "            end_acc = compute_accuracy(end_pred, end_true)\n",
    "            total_acc = (start_acc + end_acc) / 2\n",
    "\n",
    "            train_iter.set_postfix(loss=loss.item(), acc=total_acc)\n",
    "            \n",
    "            n_train += n_samples\n",
    "            train_loss += loss.item() * n_samples\n",
    "            train_acc += total_acc * n_samples\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # =============================== VALIDATION ===============================\n",
    "        model.eval()\n",
    "        \n",
    "        n_val = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                input_ids = val_batch['input_ids'].to(device)\n",
    "                attention_mask = val_batch['attention_mask'].to(device)\n",
    "                start_true = val_batch['start_positions'].to(device)\n",
    "                end_true = val_batch['end_positions'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, \n",
    "                                attention_mask=attention_mask,\n",
    "                                start_positions=start_true,\n",
    "                                end_positions=end_true)\n",
    "                start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "                end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "                assert len(start_true) == len(start_pred) == len(end_true) == len(end_pred)\n",
    "                n_samples = len(start_true)\n",
    "\n",
    "                loss = outputs['loss'].item()\n",
    "                start_acc = compute_accuracy(start_pred, start_true)\n",
    "                end_acc = compute_accuracy(end_pred, end_true)\n",
    "                total_acc = (start_acc + end_acc) / 2\n",
    "\n",
    "                n_val += n_samples\n",
    "                val_loss += loss * n_samples\n",
    "                val_acc += total_acc * n_samples\n",
    "\n",
    "        train_loss /= n_train\n",
    "        train_acc /= n_train\n",
    "        val_loss /= n_val\n",
    "        val_acc /= n_val\n",
    "\n",
    "        print(f'Epoch {epoch}: loss={train_loss:.3f}, acc={train_acc:.3f}, val_loss={val_loss:.3f}, val_acc={val_acc:.3f}')\n",
    "\n",
    "    # ================================= SAVE MODEL =============================\n",
    "    if conf['SAVE_MODEL']:\n",
    "        Path(conf['MODELS_FOLDER']).mkdir(parents=True, exist_ok=True)\n",
    "        filepath = f\"{conf['MODELS_FOLDER']}/model_{datetime.today().strftime('%m%d')}.pt\"\n",
    "        torch.save(model.state_dict(), filepath)\n",
    "        print(f'Model saved in {filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ora che la progress bar visualizza correttamente i risultati a fine epoca, non c'è bisogno del copia-incolla della cella qui sotto. Due possibilità:\n",
    "1. O si ritorna i valore di loss e accuracy a fine training e si salvano in delle variabili (per futuri training, questo ormai è andato così).\n",
    "2. O si creano due funzioni `train_loop()` e `val_loop()` e ci si ficca dentro la logica della cella qui sopra, così almeno:\n",
    "    - Si riordina il codice che è diventato abbastanza lungo.\n",
    "    - Si può richiamare solo la parte di `val_loop()` quando si ricarica il modello dopo averlo allenato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy: 0.627332936174249\n"
     ]
    }
   ],
   "source": [
    "# =============================== TESTING ===============================\n",
    "test_dataset = val_dataset  # we don't actually have the testing ds yet\n",
    "test_loader = DataLoader(test_dataset, batch_size=conf['BATCH_SIZE'])\n",
    "\n",
    "if not conf['TRAIN_MODEL']:\n",
    "    filepath = conf['MODELS_FOLDER'] + '/' + conf['MODEL_LOAD_NAME']\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    print(f\"Loaded model at {filepath}\")\n",
    "\n",
    "n_val = 0\n",
    "val_loss = 0\n",
    "val_acc = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_loader:\n",
    "        input_ids = test_batch['input_ids'].to(device)\n",
    "        attention_mask = test_batch['attention_mask'].to(device)\n",
    "        start_true = test_batch['start_positions'].to(device)\n",
    "        end_true = test_batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        start_positions=start_true,\n",
    "                        end_positions=end_true)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        assert len(start_true) == len(start_pred) == len(end_true) == len(end_pred)\n",
    "        n_samples = len(start_true)\n",
    "\n",
    "        loss = outputs['loss'].item()\n",
    "        start_acc = compute_accuracy(start_pred, start_true)\n",
    "        end_acc = compute_accuracy(end_pred, end_true)\n",
    "        total_acc = (start_acc + end_acc) / 2\n",
    "\n",
    "        n_val += n_samples\n",
    "        val_loss += loss * n_samples\n",
    "        val_acc += total_acc * n_samples\n",
    "\n",
    "val_loss /= n_val\n",
    "val_acc /= n_val\n",
    "\n",
    "print(f'Average test accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "DistilBertForQuestionAnswering                          --\n",
      "├─DistilBertModel: 1-1                                  --\n",
      "│    └─Embeddings: 2-1                                  --\n",
      "│    │    └─Embedding: 3-1                              23,440,896\n",
      "│    │    └─Embedding: 3-2                              393,216\n",
      "│    │    └─LayerNorm: 3-3                              1,536\n",
      "│    │    └─Dropout: 3-4                                --\n",
      "│    └─Transformer: 2-2                                 --\n",
      "│    │    └─ModuleList: 3-5                             42,527,232\n",
      "├─Linear: 1-2                                           1,538\n",
      "├─Dropout: 1-3                                          --\n",
      "================================================================================\n",
      "Total params: 66,364,418\n",
      "Trainable params: 66,364,418\n",
      "Non-trainable params: 0\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForQuestionAnswering                          --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           1,538\n",
       "├─Dropout: 1-3                                          --\n",
       "================================================================================\n",
       "Total params: 66,364,418\n",
       "Trainable params: 66,364,418\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50ccc516328d5b564b2da3ee1dd645a78b56fd0f0e8585996496188dec9b39eb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
