{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from config import conf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "from dataset import SquadDataset\n",
    "from torch.nn import Module\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "TRAIN_MODEL = True\n",
    "SAVE_MODEL = True\n",
    "MODELS_FOLDER = \"./models\"\n",
    "MODEL_LOAD_NAME = \"model_0125.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "dataset = SquadDataset.from_json(conf['DATASET_FILE'], tokenizer)\n",
    "train_dataset, val_dataset = dataset.train_val_split(conf['TRAIN_RATIO'])\n",
    "\n",
    "model: Module = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def compute_accuracy(pred: torch.Tensor, true: torch.Tensor) -> float:\n",
    "    assert len(pred) == len(true)\n",
    "    #TODO: check the sum\n",
    "    return ((pred == true).sum() / len(pred)).item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5475/5475 [21:10<00:00,  4.31it/s, acc=0.833, loss=0.722]\n",
      "Epoch 1: 100%|██████████| 5475/5475 [20:50<00:00,  4.38it/s, acc=0.5, loss=1.49]   \n",
      "Epoch 2: 100%|██████████| 5475/5475 [20:56<00:00,  4.36it/s, acc=0.75, loss=0.533] \n",
      "Epoch 3: 100%|██████████| 5475/5475 [20:55<00:00,  4.36it/s, acc=0.667, loss=0.524] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./models/model_0125.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    opt = Adam(model.parameters(), lr=5e-5)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=conf['BATCH_SIZE'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=conf['BATCH_SIZE'])\n",
    "\n",
    "    for epoch in range(conf['N_EPOCHS']):\n",
    "        # ================================ TRAINING ================================\n",
    "        model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "\n",
    "        train_iter = tqdm(train_loader)\n",
    "        train_iter.set_description(f'Epoch {epoch}')\n",
    "\n",
    "        for train_batch in train_iter:\n",
    "            input_ids = train_batch['input_ids'].to(device)\n",
    "            attention_mask = train_batch['attention_mask'].to(device)\n",
    "            start_true = train_batch['start_positions'].to(device)\n",
    "            end_true = train_batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            start_positions=start_true,\n",
    "                            end_positions=end_true)\n",
    "            start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "            end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "            loss = outputs['loss']\n",
    "            start_acc = compute_accuracy(start_pred, start_true)\n",
    "            end_acc = compute_accuracy(end_pred, end_true)\n",
    "            train_iter.set_postfix(loss=loss.item(),\n",
    "                                   acc=(start_acc + end_acc) / 2)\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_accs.append(start_acc)\n",
    "            train_accs.append(end_acc)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # =============================== VALIDATION ===============================\n",
    "        model.eval()\n",
    "\n",
    "        val_losses = [0]\n",
    "        val_accs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                input_ids = val_batch['input_ids'].to(device)\n",
    "                attention_mask = val_batch['attention_mask'].to(device)\n",
    "                start_true = val_batch['start_positions'].to(device)\n",
    "                end_true = val_batch['end_positions'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "                end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "                # val_losses.append(outputs['loss']) #TODO: check missing key\n",
    "                val_accs.append(compute_accuracy(start_pred, start_true))\n",
    "                val_accs.append(compute_accuracy(end_pred, end_true))\n",
    "\n",
    "        train_iter.set_postfix(loss=sum(train_losses) / len(train_losses),\n",
    "                               acc=sum(train_accs) / len(train_accs),\n",
    "                               val_loss=sum(val_losses) / len(val_losses),\n",
    "                               val_acc=sum(val_accs) / len(val_accs))\n",
    "    # SAVING\n",
    "    if SAVE_MODEL:\n",
    "        Path(MODELS_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "        filepath = f\"{MODELS_FOLDER}/model_{datetime.today().strftime('%m%d')}.pt\"\n",
    "        torch.save(model.state_dict(), filepath)\n",
    "        print(f\"Model saved in {filepath}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy: 0.627332936174249\n"
     ]
    }
   ],
   "source": [
    "# =============================== TESTING ===============================\n",
    "test_dataset = val_dataset  # we don't actually have the testing ds yet\n",
    "test_loader = DataLoader(test_dataset, batch_size=conf['BATCH_SIZE'])\n",
    "\n",
    "if not TRAIN_MODEL:\n",
    "    filepath = MODELS_FOLDER + '/' + MODEL_LOAD_NAME\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    print(f\"Loaded model at {filepath}\")\n",
    "\n",
    "model.eval()\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_loader:\n",
    "        input_ids = test_batch['input_ids'].to(device)\n",
    "        attention_mask = test_batch['attention_mask'].to(device)\n",
    "        start_true = test_batch['start_positions'].to(device)\n",
    "        end_true = test_batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        # test_losses.append(outputs['loss']) #TODO: check missing key (same as validation)\n",
    "        test_accs.append(compute_accuracy(start_pred, start_true))\n",
    "        test_accs.append(compute_accuracy(end_pred, end_true))\n",
    "\n",
    "print(f\"Average test accuracy: {sum(test_accs) / len(test_accs)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "DistilBertForQuestionAnswering                          --\n",
      "├─DistilBertModel: 1-1                                  --\n",
      "│    └─Embeddings: 2-1                                  --\n",
      "│    │    └─Embedding: 3-1                              23,440,896\n",
      "│    │    └─Embedding: 3-2                              393,216\n",
      "│    │    └─LayerNorm: 3-3                              1,536\n",
      "│    │    └─Dropout: 3-4                                --\n",
      "│    └─Transformer: 2-2                                 --\n",
      "│    │    └─ModuleList: 3-5                             42,527,232\n",
      "├─Linear: 1-2                                           1,538\n",
      "├─Dropout: 1-3                                          --\n",
      "================================================================================\n",
      "Total params: 66,364,418\n",
      "Trainable params: 66,364,418\n",
      "Non-trainable params: 0\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "================================================================================\nLayer (type:depth-idx)                                  Param #\n================================================================================\nDistilBertForQuestionAnswering                          --\n├─DistilBertModel: 1-1                                  --\n│    └─Embeddings: 2-1                                  --\n│    │    └─Embedding: 3-1                              23,440,896\n│    │    └─Embedding: 3-2                              393,216\n│    │    └─LayerNorm: 3-3                              1,536\n│    │    └─Dropout: 3-4                                --\n│    └─Transformer: 2-2                                 --\n│    │    └─ModuleList: 3-5                             42,527,232\n├─Linear: 1-2                                           1,538\n├─Dropout: 1-3                                          --\n================================================================================\nTotal params: 66,364,418\nTrainable params: 66,364,418\nNon-trainable params: 0\n================================================================================"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50ccc516328d5b564b2da3ee1dd645a78b56fd0f0e8585996496188dec9b39eb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}